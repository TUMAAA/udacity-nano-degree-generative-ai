{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql"
    },
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:13.070582985Z",
     "start_time": "2025-01-16T12:22:13.020703427Z"
    }
   },
   "cell_type": "code",
   "source": "%%sql\n",
   "id": "f2c468ca844c2a49",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f551c63a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:19.491221Z",
     "start_time": "2025-01-16T12:22:14.394345Z"
    }
   },
   "source": [
    "# Load the sms_spam dataset\n",
    "# See: https://huggingface.co/datasets/sms_spam\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
    "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=23\n",
    ")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "\n",
    "# View the dataset characteristics"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:23.661054Z",
     "start_time": "2025-01-16T12:22:23.614904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for split in splits:\n",
    "    dataset[split] = dataset[split].shuffle(seed=42).select(range(len(dataset[split])//5)) # chose a suitable sized dataset to be able to retriain quickly"
   ],
   "id": "e891ee8ccddf9396",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:25.020006Z",
     "start_time": "2025-01-16T12:22:25.010638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"The train dataset is of type: {type(dataset['train'])}\")\n",
    "print(f\"The length of the train dataset is: {len(dataset['train'])}\")\n",
    "print(f\"The length of the test dataset is: {len(dataset['test'])}\")\n",
    "print(f\"Here is one sample from the train dataset: {dataset['train'][0]=}\")"
   ],
   "id": "baef413ae920d375",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset is of type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "The length of the train dataset is: 891\n",
      "The length of the test dataset is: 223\n",
      "Here is one sample from the train dataset: dataset['train'][0]={'sms': \"How would my ip address test that considering my computer isn't a minecraft server\\n\", 'label': 0}\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e835bda6327b6ce0"
  },
  {
   "cell_type": "code",
   "id": "4935cb4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:29.963034Z",
     "start_time": "2025-01-16T12:22:29.657262Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Using map() inspired by the example provided inside the Udacity course\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "# Inspect the available columns in the dataset\n",
    "print(f\"The tokenized train dataset is of type: {type(tokenized_dataset['train'])}\")\n",
    "print(f\"The length of the tokenized train dataset is: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Here is one sample from the train dataset: {tokenized_dataset['train'][0]=}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tumaaa/work/repos/udacity_nano_degree_generative_ai/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenized train dataset is of type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "The length of the tokenized train dataset is: 891\n",
      "Here is one sample from the train dataset: tokenized_dataset['train'][0]={'sms': \"How would my ip address test that considering my computer isn't a minecraft server\\n\", 'label': 0, 'input_ids': [101, 2129, 2052, 2026, 12997, 4769, 3231, 2008, 6195, 2026, 3274, 3475, 1005, 1056, 1037, 3067, 10419, 8241, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "f28c4a78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:32.876012Z",
     "start_time": "2025-01-16T12:22:32.526680Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "def load_not_retrained_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2,\n",
    "        id2label={0: \"not spam\", 1: \"spam\"},\n",
    "        label2id={\"not spam\": 0, \"spam\": 1},\n",
    "    )\n",
    "\n",
    "model = load_not_retrained_model()\n",
    "\n",
    "# Since we will not retrain the base model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tumaaa/work/repos/udacity_nano_degree_generative_ai/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "019b9f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:37.223044Z",
     "start_time": "2025-01-16T12:22:37.216461Z"
    }
   },
   "source": [
    "# Inspired by the example provided inside the Udacity course\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:37.900759Z",
     "start_time": "2025-01-16T12:22:37.891542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "# We really just need the trainer for easier evaluation :)\n",
    "# Packed into a function to allow reloading if the notebook is restarted\n",
    "def create_trainer(model, tokenizer, tokenized_dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./data/sms_spam_classifier\",\n",
    "            learning_rate=2e-3,\n",
    "            # Reduce the batch size if you don't have enough memory\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer"
   ],
   "id": "4c67c7a041f7a629",
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "5176b07f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:45.822826Z",
     "start_time": "2025-01-16T12:22:39.695659Z"
    }
   },
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "trainer = create_trainer(model, tokenizer, tokenized_dataset)\n",
    "trainer.evaluate()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:04]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6778894066810608,\n",
       " 'eval_accuracy': 0.7623318385650224,\n",
       " 'eval_runtime': 6.0641,\n",
       " 'eval_samples_per_second': 36.774,\n",
       " 'eval_steps_per_second': 9.235}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "id": "5775fadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:48.143592Z",
     "start_time": "2025-01-16T12:22:47.931306Z"
    }
   },
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, AutoPeftModelForSequenceClassification\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"], # Very important. We just provide Lora lasyers for the attention part of the model for quick retraining though with great results as evident at the end\n",
    "    inference_mode=False\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,479,172 || all params: 67,842,052 || trainable%: 2.180317305260755\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "894046c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:22:54.674687Z",
     "start_time": "2025-01-16T12:22:48.737093Z"
    }
   },
   "source": [
    "trainer_lora = create_trainer(lora_model, tokenizer, tokenized_dataset)\n",
    "print(\"We will first evaluate how good/bad the lora model before retraining is\")\n",
    "trainer_lora.evaluate()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='112' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:56]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6778894066810608,\n",
       " 'eval_accuracy': 0.7623318385650224,\n",
       " 'eval_runtime': 5.8966,\n",
       " 'eval_samples_per_second': 37.818,\n",
       " 'eval_steps_per_second': 9.497}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "c4d4c908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:24:45.169419Z",
     "start_time": "2025-01-16T12:22:54.730917Z"
    }
   },
   "source": [
    "print(\"Now we retrain the selected layers of the Lora model\")\n",
    "trainer_lora.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='223' max='223' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [223/223 01:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.207639</td>\n",
       "      <td>0.950673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/sms_spam_classifier/checkpoint-223 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=223, training_loss=0.2891566528867713, metrics={'train_runtime': 110.3047, 'train_samples_per_second': 8.078, 'train_steps_per_second': 2.022, 'total_flos': 10373903680320.0, 'train_loss': 0.2891566528867713, 'epoch': 1.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "b47abf88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:24:51.075932Z",
     "start_time": "2025-01-16T12:24:45.248731Z"
    }
   },
   "source": [
    "print(\"Now we reevaluate the Lora model after retrainig\")\n",
    "trainer_lora.evaluate()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:05]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.20763860642910004,\n",
       " 'eval_accuracy': 0.9506726457399103,\n",
       " 'eval_runtime': 5.8135,\n",
       " 'eval_samples_per_second': 38.359,\n",
       " 'eval_steps_per_second': 9.633,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f465122497af2eaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:25:46.629358Z",
     "start_time": "2025-01-16T12:25:46.593883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sms_spam_classifier_lora_refined_save_location = \"sms_spam_classifier_lora_refined\"\n",
    "lora_model.save_pretrained(sms_spam_classifier_lora_refined_save_location)\n",
    "print(f\"saved the trained Lora model in {sms_spam_classifier_lora_refined_save_location}\")"
   ],
   "id": "e0c8df6d8ef8b98e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved the trained Lora model in sms_spam_classifier_lora_refined\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "id": "863ec66e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:26:19.408135Z",
     "start_time": "2025-01-16T12:26:19.399962Z"
    }
   },
   "source": [
    "test_sample=dataset[\"test\"].shuffle().select(range(10))\n",
    "print(\"A test sample of size {len(test_sample)} was chosen\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A test sample of size {len(test_sample)} was chosen\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "bc3a8147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:26:22.217515Z",
     "start_time": "2025-01-16T12:26:21.013862Z"
    }
   },
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "lora_model=AutoPeftModelForSequenceClassification.from_pretrained(sms_spam_classifier_lora_refined_save_location)\n",
    "\n",
    "if 'model' not in locals():\n",
    "    model = load_not_retrained_model()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tumaaa/work/repos/udacity_nano_degree_generative_ai/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "bc96905a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T12:26:32.356936Z",
     "start_time": "2025-01-16T12:26:31.643293Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "inputs = tokenizer(test_sample[\"sms\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "results_lora=lora_model(**inputs).logits.argmax(axis=1)\n",
    "results_not_retrained=model(**inputs).logits.argmax(axis=1)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"sms\": [item[\"sms\"] for item in test_sample],\n",
    "    \"predictions_lora\": results_lora,\n",
    "    \"predictions_not_retrained\": results_not_retrained,\n",
    "    \"label\": test_sample[\"label\"]\n",
    "})\n",
    "\n",
    "print(df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                sms  \\\n",
      "0  Love isn't a decision, it's a feeling. If we could decide who to love, then, life would be much simpler, but then less magical\\n   \n",
      "1                                                                 I'm working technical support :)voice process.networking field.\\n   \n",
      "2                                                                                Whats the staff name who is taking class for us?\\n   \n",
      "3                                                                                          Awesome, text me when you're restocked\\n   \n",
      "4                                                                                            Me i'm not workin. Once i get job...\\n   \n",
      "5                                                         Lol yes. Our friendship is hanging on a thread cause u won't buy stuff.\\n   \n",
      "6                                                                                        Die... Now i have e toot fringe again...\\n   \n",
      "7                                                            Eat at old airport road... But now 630 oredi... Got a lot of pple...\\n   \n",
      "8                                                  K ill drink.pa then what doing. I need srs model pls send it to my mail id pa.\\n   \n",
      "9                                                                             Indeed and by the way it was either or - not both !\\n   \n",
      "\n",
      "   predictions_lora  predictions_not_retrained  label  \n",
      "0                 0                          0      0  \n",
      "1                 0                          0      0  \n",
      "2                 0                          0      0  \n",
      "3                 0                          0      0  \n",
      "4                 0                          0      0  \n",
      "5                 0                          0      0  \n",
      "6                 0                          0      0  \n",
      "7                 1                          1      0  \n",
      "8                 0                          0      0  \n",
      "9                 0                          0      0  \n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "id": "866ab28c",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9a32e4e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
